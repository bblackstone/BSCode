{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install Required Libraries**"
      ],
      "metadata": {
        "id": "VHnTzUQbpHAs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSLaTlk9k7MH"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets accelerate evaluate torch\n",
        "!pip install -U -q transformers timm\n",
        "!pip install -q rouge_score jiwer\n",
        "!pip install -q peft\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q trl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Obtain Hugging Face Access**"
      ],
      "metadata": {
        "id": "DGhtySG3pQ6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grant access to hugging face\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n"
      ],
      "metadata": {
        "id": "vIgWpCbip1OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare the Dataset**"
      ],
      "metadata": {
        "id": "L0j7nSxXpOO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('Nan-Do/instructional_code-search-net-java')\n",
        "\n",
        "# Check the column names\n",
        "print(dataset['train'].column_names)"
      ],
      "metadata": {
        "id": "kc1DKfQUq-Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Model and Tokenizer**"
      ],
      "metadata": {
        "id": "P5EVKOUFwiJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from accelerate import PartialState\n",
        "\n",
        "# Load the Starcoder model and tokenizer\n",
        "\n",
        "model_name = \"bigcode/starcoder2-3b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=HF_TOKEN)\n",
        "\n",
        "# Set for 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "\n",
        "# Load the model with the specified BitsAndBytesConfig\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"bigcode/starcoder2-3b\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map={\"\": PartialState().process_index}\n",
        "    )"
      ],
      "metadata": {
        "id": "PEDLi8gr9FSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a padding token to the tokenizer before preprocessing\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "# Update model embeddings to include the new padding token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Tokenizing the inputs (Instruction column)\n",
        "    inputs = tokenizer(examples['INSTRUCTION'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "    # Tokenizing the responses (this will be used as labels for supervised learning)\n",
        "    # We shift the labels by one position to the right\n",
        "    # because we want the model to predict the next token in the sequence\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples['RESPONSE'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels list.\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    # We want to ignore padding tokens in the loss, so we set them to -100\n",
        "    inputs[\"labels\"] = [\n",
        "        -100 if token == tokenizer.pad_token_id else token for token in inputs[\"labels\"]\n",
        "    ]\n",
        "\n",
        "    return inputs\n",
        "# Apply preprocessing\n",
        "train_dataset = dataset['train'].map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "n3F_RrsOuahy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_eval_split = train_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_eval_split['train']\n",
        "eval_dataset = train_eval_split['test']"
      ],
      "metadata": {
        "id": "NjLxTRiD5fVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Up Training Arguments**"
      ],
      "metadata": {
        "id": "U6M_VA1Lxfrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Model\n",
        "    parser.add_argument(\"--model_id\", type=str, default=\"bigcode/starcoder2-3b\")  # Default to StarCoder2-3b\n",
        "\n",
        "    # Training Parameters\n",
        "    parser.add_argument(\"--max_seq_length\", type=int, default=512)  # Maximum sequence length for input and labels\n",
        "    parser.add_argument(\"--max_steps\", type=int, default=2000)  # Total training steps\n",
        "    parser.add_argument(\"--micro_batch_size\", type=int, default=1)  # Batch size per device\n",
        "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=4)  # Gradient accumulation steps\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=0.01)  # Weight decay for regularization\n",
        "    parser.add_argument(\"--bf16\", type=bool, default=True)  # Use bfloat16 precision\n",
        "\n",
        "    # Optimizer and Learning Rate\n",
        "    parser.add_argument(\"--attention_dropout\", type=float, default=0.1)  # Dropout for attention layers\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=2e-4)  # Learning rate\n",
        "    parser.add_argument(\"--lr_scheduler_type\", type=str, default=\"cosine\")  # Learning rate scheduler type\n",
        "    parser.add_argument(\"--warmup_steps\", type=int, default=100)  # Warmup steps for learning rate\n",
        "\n",
        "    # Other Settings\n",
        "    parser.add_argument(\"--seed\", type=int, default=0)  # Random seed for reproducibility\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"./starcoder_finetuned\")  # Output directory for saving the model\n",
        "    parser.add_argument(\"--num_proc\", type=int, default=None)  # Number of processes for data preprocessing (if applicable)\n",
        "    parser.add_argument(\"--push_to_hub\", type=bool, default=False)  # Whether to push the model to the Hugging Face Hub\n",
        "\n",
        "    # Parse the arguments\n",
        "    args = parser.parse_args([])\n",
        "    return args"
      ],
      "metadata": {
        "id": "P1lF9vVZwr5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import TrainingArguments\n",
        "\n",
        "# # Training Arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./starcoder_finetuned\",  # Directory to save model outputs\n",
        "#     num_train_epochs=30,                  # Number of training epochs\n",
        "#     per_device_train_batch_size=1,       # Batch size per GPU (adjust based on memory)\n",
        "#     gradient_accumulation_steps=8,       # Accumulate gradients to simulate larger batch size\n",
        "#     evaluation_strategy=\"steps\",         # Evaluate during training\n",
        "#     save_strategy=\"steps\",               # Save model checkpoints during training\n",
        "#     logging_dir=\"./logs\",                # Directory for logs\n",
        "#     logging_steps=50,                    # Log every 50 steps\n",
        "#     save_steps=500,                      # Save model every 500 steps\n",
        "#     learning_rate=5e-5,                  # Learning rate\n",
        "#     weight_decay=0.01,                   # Weight decay\n",
        "#     fp16=True,                           # Use mixed precision for faster training\n",
        "#     push_to_hub=False                    # Set to True if pushing the model to Hugging Face Hub\n",
        "# )"
      ],
      "metadata": {
        "id": "PjvbFT_Nxkkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Metrics**"
      ],
      "metadata": {
        "id": "x3l0QvpI0nSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "\n",
        "# Load various metrics\n",
        "metrics = {\n",
        "    \"accuracy\": load(\"accuracy\"),\n",
        "    \"perplexity\": load(\"perplexity\"),\n",
        "    \"rouge\": load(\"rouge\"),\n",
        "    \"bleu\": load(\"bleu\"),\n",
        "    \"f1\": load(\"f1\"),\n",
        "    \"precision\": load(\"precision\"),\n",
        "    \"recall\": load(\"recall\"),\n",
        "    \"meteor\": load(\"meteor\"),\n",
        "    \"wer\": load(\"wer\"),\n",
        "}\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Initialize a dictionary to store the results\n",
        "    results = {}\n",
        "\n",
        "    # Compute metrics for the decoded outputs\n",
        "    for metric_name, metric in metrics.items():\n",
        "        if metric_name == \"accuracy\":\n",
        "            # For accuracy, compute using the predicted indices\n",
        "            # predictions is usually logits, so we need to use argmax to get the predicted token ids\n",
        "            pred_ids = predictions.argmax(axis=-1)\n",
        "            results[metric_name] = metric.compute(predictions=pred_ids, references=labels)\n",
        "        else:\n",
        "            # For other metrics like ROUGE, BLEU, we use the decoded text\n",
        "            results[metric_name] = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "uZ9DhBvnz_2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Trainer**"
      ],
      "metadata": {
        "id": "nnERi6v90rFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        target_modules=[\n",
        "            \"q_proj\",\n",
        "            \"o_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"gate_proj\",\n",
        "            \"up_proj\",\n",
        "            \"down_proj\",\n",
        "        ],\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "# Apply the PEFT model with QLoRA\n",
        "model = get_peft_model(model, lora_config)\n"
      ],
      "metadata": {
        "id": "bzokKSgsBiS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "import transformers\n",
        "\n",
        "args = get_args()\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    # dataset_text_field=\"RESPONSE\",  # Removed: This argument is deprecated\n",
        "    # Instead, pass the text column name to formatting_func:\n",
        "    formatting_func=lambda examples: tokenizer(examples[\"RESPONSE\"], padding=\"max_length\", truncation=True, max_length=args.max_seq_length),\n",
        "    # max_seq_length=args.max_seq_length,  # Removed: This argument is no longer needed in SFTTrainer\n",
        "    peft_config=lora_config,  # LoRA configuration\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=args.micro_batch_size,\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        warmup_steps=args.warmup_steps,\n",
        "        max_steps=args.max_steps,\n",
        "        learning_rate=args.learning_rate,\n",
        "        lr_scheduler_type=args.lr_scheduler_type,\n",
        "        weight_decay=args.weight_decay,\n",
        "        bf16=args.bf16,  # Use bf16 precision\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=10,  # Log every 10 steps\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=100,  # Evaluate every 100 steps\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,  # Save every 100 steps\n",
        "        output_dir=args.output_dir,  # Output directory\n",
        "        optim=\"paged_adamw_8bit\",  # Optimizer\n",
        "        seed=args.seed,  # Random seed\n",
        "        run_name=f\"train-{args.model_id.split('/')[-1]}\",  # Run name\n",
        "        report_to=\"wandb\",  # Report to Weights & Biases (if installed)\n",
        "        load_best_model_at_end=True,  # Load the best model at the end\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "R7ayTQjozqkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Start Fine-Tuning**"
      ],
      "metadata": {
        "id": "qkGPCW8I0wRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using accelerator to offload certain parts\n",
        "# from accelerate import Accelerator\n",
        "# from torch.utils.data import DataLoader\n",
        "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "# accelerator = Accelerator()\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
        "# model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "RttzfAgn0HUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the model**"
      ],
      "metadata": {
        "id": "HD6dPhek03_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation set\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Print all the evaluation metrics\n",
        "print(\"Evaluation results:\", eval_results)\n"
      ],
      "metadata": {
        "id": "xvRZo7BX01rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Push the Model into Hugging Face**"
      ],
      "metadata": {
        "id": "gXjaKdKq3k-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Push my fine tuned model with its tokenizer to my hugging face repo\n",
        "repo_path=\"BSAtlas/BSCode-1-Stable\"\n",
        "model.push_to_hub(\n",
        "   repo_path,\n",
        "   token=HF_TOKEN\n",
        ")\n",
        "tokenizer.push_to_hub(\n",
        "  repo_path,\n",
        "  token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "vkgOReI43p2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_name = \"./starcoder_finetuned\"  # Path to your fine-tuned model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Function to generate documentation and optimize Java code\n",
        "def generate_documentation_and_optimization(input_text, max_length=1024):\n",
        "    # Tokenize input text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "\n",
        "    # Ensure the model runs on the right device (GPU if available)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # Generate output tokens (with beam search, greedy search, etc.)\n",
        "    output = model.generate(**inputs, max_length=max_length, num_return_sequences=1, top_k=50, top_p=0.95, temperature=0.7)\n",
        "\n",
        "    # Decode the generated tokens back to text\n",
        "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return decoded_output\n",
        "\n",
        "# Java code to pass to the model\n",
        "input_code = \"\"\"\n",
        "Given the following Java code, first generate the documentation in the form of comments for the class and methods. Then, optimize the code to improve efficiency or readability:\n",
        "\n",
        "public class FactorialCalculator {\n",
        "    public static int calculateFactorial(int n) {\n",
        "        if (n == 0) return 1;\n",
        "        return n * calculateFactorial(n - 1);\n",
        "    }\n",
        "\n",
        "    public static void main(String[] args) {\n",
        "        int number = 5;\n",
        "        System.out.println(\"Factorial of \" + number + \" is \" + calculateFactorial(number));\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Get the model's response\n",
        "response = generate_documentation_and_optimization(input_code)\n",
        "\n",
        "# Print the generated documentation and optimized code\n",
        "print(\"Generated Documentation and Optimized Code:\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "p3ghWwGY-xNv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}